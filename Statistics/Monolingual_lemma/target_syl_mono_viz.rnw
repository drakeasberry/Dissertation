% Monolingual Online Segmentation Visualization code

<<global_opts_mono, echo=TRUE, cache=FALSE>>=
library(knitr)
library(here)

knitr::opts_chunk$set(
  echo = FALSE
)
here::here()
set_parent(here('Asberry_Dissertation/Dissertation.Rnw'))
@


<<gen_mono_lib, echo = FALSE>>=
# Load Libraries
library(tidyverse)
library(ggplot2)
library(cowplot)
library(effsize)
library(xtable)

# Source Scripts containing functions
source('../../Scripts_Dissertation/diss_dataviz_script.R')
source('../../Scripts_Dissertation/analysis_functions.R')
@

<<mono_participants, echo = FALSE>>=
# Read in data file
my_l1_data <- 
  read_csv('analyze_data/output/44_online_natives_segmentation.csv')

# Read in raw file of all segmentation data
my_online_l1_raw_data <- 
  read_csv('analyze_data/output/online_L1_raw_data.csv') %>% 
  subset(., partNum %in% my_l1_data$partNum)

# Delete practice trial rows
online_l1_exper_trials <- drop_na(my_online_l1_raw_data, 
                                  any_of("corrAns"))
# Critical trials 
my_online_l1_critical_data <- 
  read_csv('analyze_data/output/online_L1_critical_data.csv') %>% 
  subset(., partNum %in% my_l1_data$partNum)

my_online_l1_critical_correct <- my_online_l1_critical_data %>% 
  subset(., segResp == 'b')

my_online_l1_under_200 <- my_online_l1_critical_correct %>% 
  subset(., segRespRTmsec > 200)

my_online_l1_over_1500 <- my_online_l1_under_200 %>% 
  subset(., segRespRTmsec < 1500)
@

<<mono_data, echo = FALSE>>=
# Transform data in long form with 1 row per participant per condition
# List columns to group by
online_l1_grouping <- c("partNum", 
                        "group", 
                        "word_status", 
                        "word_initial_syl", 
                        "target_syl_structure", 
                        "matching")

# Aggregaates and transforms data into long form
# Adds columns for median of RT in msec and log
online_natives <- trans_long(my_l1_data, online_l1_grouping) 

online_example_cond <- online_natives %>% 
  subset(., partNum == 'part200') %>% 
  rename(Participant = partNum,
         Target = target_syl_structure,
         Matching = matching,
         Lexicality = word_status,
         RT_log_median = median_RTlog) %>% 
  ungroup() %>% 
  select(Participant, Target, Matching, Lexicality, RT_log_median)

# Get summary stats for reporting 
# Columns that I want summary stats for
online_l1_stat_col <- c("segRespRTmsec",
                        "log_RT")

# Stats I want to run
online_l1_summary_stats <- quote(list(Median = median,
                            Mean = mean, 
                            Min = min, 
                            Max = max, 
                            SD = sd))

# Grouping by group and condition
online_l1_cond_grp <- c("group",
              "target_syl_structure",
              "word_initial_syl",
              "matching", 
              "word_status")

# grouped by participant and condition
online_l1_ind_sum <- my_stats(my_l1_data, 
              grp_col = online_l1_grouping, 
              sum_col = online_l1_stat_col,
              stats = online_l1_summary_stats)

# grouped by group and condition
online_l1_grp_sum <- my_stats(my_l1_data, 
              grp_col = online_l1_cond_grp, 
              sum_col = online_l1_stat_col,
              stats = online_l1_summary_stats) %>% 
  mutate(index = paste("Spanish",
                       target_syl_structure,
                       word_initial_syl,
                       word_status, sep = "-")) %>%
  mutate_at(vars(ends_with("_Mean")), 
              list(~ round(., 2))) %>% 
  mutate_at(vars(ends_with("_SD")), 
              list(~ round(., 2))) %>% 
  mutate_at(vars(ends_with("_Median")), 
              list(~ round(., 2))) %>%
  mutate_at(vars(ends_with("_Min")), 
              list(~ round(., 2))) %>% 
  mutate_at(vars(ends_with("_Max")), 
              list(~ round(., 2))) %>% 
  column_to_rownames(., var = "index")

# Get some average RTs
online_l1_grp_sum %>% 
  select(., c(word_status, segRespRTmsec_Mean)) %>% 
  View()

online_l1_wd_av <- online_l1_grp_sum %>% 
  subset(., word_status == "word") %>% 
  select(., c(word_status, segRespRTmsec_Mean))

online_l1_nonwd_av <- online_l1_grp_sum %>% 
  subset(., word_status != "word") %>% 
  select(., c(word_status, segRespRTmsec_Mean))

# Summary statistics for reaction time
# Group data by target syllable structure, matching condition, and word status, 
online_l1_grouping_stats <- c("target_syl_structure", 
                              "matching", 
                              "word_status")

# Online Natives
bxp_online_natives <- online_natives %>% 
  mutate(wd_new = factor(word_status, 
                         levels = c("word", "nonword"),
                         labels = c("Word", "Nonword")),
         mat_new = factor(matching, 
                          levels = c("match", "mismatch"),
                          labels = c("Matching", "Mismatching"))) %>% 
  ggplot(aes(x = target_syl_structure,
             y = median_RTmsec,
             color = mat_new)) +
  geom_boxplot(fill = NA, position = position_dodge(1)) +
  geom_violin(fill = NA, position = position_dodge(1)) +
  facet_grid(. ~ wd_new) +
  labs(title = "L1 Spanish by Target Syllable",
       x = "Target Syllable Structure",
       y = "Reaction Time (msec)") +
  scale_color_manual(name = "Condition", values = plt_color_2)
bxp_online_natives 


# Check for outliers
online_outlier_native <- online_natives %>% 
  outlier_chk(., online_l1_grouping_stats, "median_RTmsec")

# Check for normality
online_normality_native <- online_natives %>% 
  normality_chk(., online_l1_grouping_stats, "median_RTmsec")


# Create QQ plots
# Natives by milliseconds
ggqqplot(
  online_natives, 
  "median_RTmsec", 
  ggtheme = theme_bw(), 
  title = "QQ Plot by Target Syllable in Milliseconds by L1 Spanish Speakers") +
  facet_grid(target_syl_structure + matching ~ word_status, 
             labeller = "label_both") 

# Natives by log values
ggqqplot(
  online_natives, 
  "median_RTlog",
  ggtheme = theme_bw(),
  title = "QQ Plot by Target Syllable in log by L1 Spanish Speakers") +
  facet_grid(target_syl_structure + matching ~ word_status, 
             labeller = "label_both")


# Run inferential statistics Native group
# Run 3 way repeated measures anova
aov_online_natives <- aov_ez("partNum", 
                             "median_RTlog", 
                             online_natives, 
                             within = c(online_l1_grouping_stats))
aov_online_natives
## main effect of matching condition 
## main effect trending for word status
## no signficant interactions

# Create Manuscript Version
online_grouping_pretty <- c("Target", "Matching", "Lexicality")
online_aov_natives_prt <- online_natives %>% 
  rename(., c(Target = target_syl_structure,
              Matching = matching,
              Lexicality = word_status)) %>% 
  aov_ez("partNum", 
         "median_RTlog", 
         ., 
         within = c(online_grouping_pretty))
online_aov_natives_prt

# Regroup to run paired t-test for matching/mismatching over all other conditions
online_l1_grouping_mat <- 
  online_l1_grouping[! online_l1_grouping %in% 
                       c("word_status", 
                         "word_initial_syl",
                         "target_syl_structure", 
                         "group")]

# Aggregate data by matching condition
online_l1_mat_ag <- my_l1_data %>% 
  my_stats(., grp_col = online_l1_grouping_mat,
           sum_col = online_l1_stat_col,
           stats = online_l1_summary_stats)


# Significant Main Effect Exploration
# Natives t.test for matching condition
t_online_l1_mat_main <- t.test(
  online_l1_mat_ag$log_RT_Median ~ 
    online_l1_mat_ag$matching, 
  paired = TRUE, 
  alternative = "less")
t_online_l1_mat_main
## is significant t = -2.929, df = 43, p-value = 0.002711

# Descriptives to check direction of effect
with(online_l1_mat_ag, 
     tapply(log_RT_Median, 
            matching, 
            FUN = mean))
## matching condition responded to faster than mismatching condition

# Create a table for referencing statistics with CIs
online_l1_mat_compare <- online_l1_mat_ag  %>% 
  group_by(matching) %>% 
  summarise(Mean = mean(log_RT_Median),
            SD = sd(log_RT_Median),
            N = n(),
            MOE_95 = qnorm(0.975)*SD/sqrt(N-1),
            LL = Mean - MOE_95,
            UL = Mean + MOE_95) %>% 
  column_to_rownames(., var = "matching")

# Calculate Cohen's D to estimate effect size
online_l1_mat_efsize <- 
  cohen.d(online_l1_mat_ag$log_RT_Median, 
          online_l1_mat_ag$matching, 
          na.rm = TRUE, 
          paired = TRUE)
online_l1_mat_efsize
# d estimate: -0.2008373 (small)

# Significant Main Effect Trending Exploration
# Regroup to run paired t-test for word status over all other conditions 
online_l1_grouping_lex <- 
  online_l1_grouping[! online_l1_grouping %in% 
                       c("target_syl_structure", 
                         "matching", 
                         "word_initial_syl",
                         "group")]

# Aggregate data on word status
online_l1_lex_ag <- my_l1_data %>% 
  my_stats(., grp_col = online_l1_grouping_lex,
           sum_col = online_l1_stat_col,
           stats = online_l1_summary_stats)

# Natives t.test for word status (trending)
t_online_l1_lex_main <- t.test(
  online_l1_lex_ag$log_RT_Median ~ 
    online_l1_lex_ag$word_status, 
  paired = TRUE)
t_online_l1_lex_main
## is not significant t = 1.3885, df = 43, p-value = 0.172

# Descriptives to check direction of effect
with(online_l1_lex_ag, 
     tapply(log_RT_Median, 
            word_status, 
            FUN = mean))
## words responded to faster than nonwords, but not significantly so

# Create a table for referencing statistics with CIs
online_l1_lex_compare <- online_l1_lex_ag %>% 
  group_by(word_status) %>% 
  summarise(Mean = mean(log_RT_Median),
            SD = sd(log_RT_Median),
            N = n(),
            MOE_95 = qnorm(0.975)*SD/sqrt(N-1),
            LL = Mean - MOE_95,
            UL = Mean + MOE_95) %>% 
  column_to_rownames(., var = "word_status")

# Get the size of the effect
online_l1_lex_efsize <- 
  cohen.d(online_l1_lex_ag$log_RT_Median, 
          online_l1_lex_ag$word_status, 
          na.rm = TRUE, 
          paired = TRUE)
online_l1_lex_efsize
# d estimate: 0.1158181 (negligible) crosses 0 

# Native plots
# Main effect of matching condition
online_l1_mat_main <- afex_plot(
  aov_online_natives, 
  x = "matching", 
  error = "within",
  factor_levels = list(
    matching = c(match = "Matching", 
                 mismatch = "Mismatching")),
  data_plot = FALSE,
  mapping = c("color", "shape")) +
  labs(title = "Estimated Marginal Means L1 Spanish",
       x = "Matching Condition",
       y = "Reaction Time (log)") +
  scale_color_manual(values = plt_color_2) +
  theme(legend.position = "none")
online_l1_mat_main

# Main effect of lexicality
online_l1_lex_main <- afex_plot(
  aov_online_natives, 
  x = "word_status", 
  error = "within",
  factor_levels = list(
    word_status = c(word = "Word", 
                    nonword = "Noword")),
  data_plot = FALSE,
  mapping = c("color", "shape")) +
  labs(title = "Estimated Marginal Means L1 Spanish",
       x = "Lexicality",
       y = "Reaction Time (log)") +
  scale_color_manual(values = plt_color_2) +
  theme(legend.position = "none")
online_l1_lex_main

# Interaction breakdown
## No significant interactions to explore
# NS Interaction between target and matching condition
# with lexicality in two panels
online_l1_tar_mat_int <- afex_plot(
  aov_online_natives, 
  x = "target_syl_structure", 
  trace = "matching",
  panel = "word_status",
  error = "within",
  mapping = c("shape", "color", "linetype"),
  factor_levels = list(
    word_status = c(word = "Word", nonword = "Noword"),
    matching = c("Matching", "Mismatching")),
  legend_title = "Condition",
  data_plot = FALSE) +
  labs(title = "Estimated Marginal Means L1 Spanish",
       x = "Target Syllable Structure",
       y = "Reaction Time (log)") +
  scale_color_manual(values = plt_color_2)
online_l1_tar_mat_int

# Remove temporary data variables in environment
# Remove dataframes following analysis
rm(online_l1_lex_ag)

# Remove checks and unused plots
rm(online_normality_native,
   online_outlier_native)

unique(my_l1_data$segRespCorr)
word_resp <- my_l1_data %>% 
  subset(., word_status == 'word')
mean(word_resp$segRespRTmsec)

nonword_resp <- my_l1_data %>% 
  subset(., word_status != 'word')
mean(nonword_resp$segRespRTmsec)

# Run stats per previous study analysis
# Interaction breakdown Words and Nonwords
historic_grouping_pretty <- c("Target", "Carrier", "Lexicality")
historic_aov_natives_prt <- online_natives %>% 
  rename(., c(Target = target_syl_structure,
              Carrier = word_initial_syl,
              Lexicality = word_status)) %>% 
  aov_ez("partNum", 
         "median_RTlog", 
         ., 
         within = c(historic_grouping_pretty))
historic_aov_natives_prt

# plot words and nonwords interaction
# comment panel arg for combined plot
# uncomment panel arg for separate Word/Nonword panels 
historic_l1_tar_car_int <- afex_plot(
  historic_aov_natives_prt, 
  x = "Target", 
  trace = "Carrier",
  panel = "Lexicality",
  error = "within",
  mapping = c("shape", "color", "linetype"),
  factor_levels = list(
    Lexicality = c(word = "Word", nonword = "Noword"),
    Carrier = c("CV", "CVC")),
  legend_title = "Carrier Syllable Structure",
  data_plot = FALSE) +
  labs(title = "Estimated Marginal Means L1 Spanish",
       x = "Target Syllable Structure",
       y = "Reaction Time (log)") +
  scale_color_manual(values = plt_color_2)
historic_l1_tar_car_int

# Interaction breakdown Words only
online_natives_wd <- online_natives %>% 
  subset(., word_status == "word")
historic_grouping_pretty_wd <- c("Target", "Carrier")
historic_aov_natives_prt_wd <- online_natives_wd %>% 
  rename(., c(Target = target_syl_structure,
              Carrier = word_initial_syl)) %>% 
  aov_ez("partNum", 
         "median_RTlog", 
         ., 
         within = c(historic_grouping_pretty_wd))
historic_aov_natives_prt_wd

# plot the word data interaction
historic_l1_tar_car_int_wd <- afex_plot(
  historic_aov_natives_prt_wd, 
  x = "Target", 
  trace = "Carrier",
  error = "within",
  mapping = c("shape", "color", "linetype"),
  factor_levels = list(
    Carrier = c("CV", "CVC")),
  legend_title = "Carrier Syllable Structure",
  data_plot = FALSE) +
  labs(title = "Estimated Marginal Means L1 Spanish",
       x = "Target Syllable Structure",
       y = "Reaction Time (log)") +
  scale_color_manual(values = plt_color_2)
historic_l1_tar_car_int_wd


# Save tabled data
# https://tex.stackexchange.com/questions/481802/reporting-r-results-in-latex

# Save all analyses
#out_dir = 'analyze_data/output/figures/by_target/'

# Save plots
#ggsave('online_natives_descriptive_data.png', bxp_online_natives, 'png', out_dir)
#ggsave('online_natives_mat_main.png', l1_mat_main, 'png', out_dir)
@