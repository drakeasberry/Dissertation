{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='./data/temp_data/nonwords_all.csv' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import segFunctions\n",
    "\n",
    "# runs the csv_from_excel function:\n",
    "# creates csv for lexical access experiment lists\n",
    "segFunctions.csv_from_excel('Lexical_Access_Experimental_Item_Setup.xlsx','All_LexExperiment_Lists','./data/original_data/exp_files/expLists_all.csv')\n",
    "\n",
    "#creates a csv for pulling out random selections by word\n",
    "segFunctions.csv_from_excel('LexicalNonwordStats_Wuggy.xlsx','WuggyBuilt','./data/temp_data/nonwords_all.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Match</th>\n",
       "      <th>Nonword</th>\n",
       "      <th>Nonword_Concat</th>\n",
       "      <th>Rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>vol-can</td>\n",
       "      <td>fel-can</td>\n",
       "      <td>felcan</td>\n",
       "      <td>felcan</td>\n",
       "      <td>0.166801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>vol-can</td>\n",
       "      <td>fil-can</td>\n",
       "      <td>filcan</td>\n",
       "      <td>filcan</td>\n",
       "      <td>0.862816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>vol-can</td>\n",
       "      <td>fol-dan</td>\n",
       "      <td>foldan</td>\n",
       "      <td>foldan</td>\n",
       "      <td>0.458116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>vol-can</td>\n",
       "      <td>fop-can</td>\n",
       "      <td>fopcan</td>\n",
       "      <td>fopcan</td>\n",
       "      <td>0.249141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>vol-can</td>\n",
       "      <td>vuz-can</td>\n",
       "      <td>vuzcan</td>\n",
       "      <td>vuzcan</td>\n",
       "      <td>0.032408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word    Match Nonword Nonword_Concat      Rand\n",
       "795  vol-can  fel-can  felcan         felcan  0.166801\n",
       "796  vol-can  fil-can  filcan         filcan  0.862816\n",
       "797  vol-can  fol-dan  foldan         foldan  0.458116\n",
       "798  vol-can  fop-can  fopcan         fopcan  0.249141\n",
       "799  vol-can  vuz-can  vuzcan         vuzcan  0.032408"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/temp_data/nonwords_all.csv')\n",
    "i = 0\n",
    "j = 9\n",
    "nonwords = []\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Match</th>\n",
       "      <th>Nonword</th>\n",
       "      <th>Nonword_Concat</th>\n",
       "      <th>Rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>se-gu-ro</td>\n",
       "      <td>te-ru-to</td>\n",
       "      <td>teruto</td>\n",
       "      <td>teruto</td>\n",
       "      <td>0.129886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>se-gu-ro</td>\n",
       "      <td>te-ru-no</td>\n",
       "      <td>teruno</td>\n",
       "      <td>teruno</td>\n",
       "      <td>0.143047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>se-gu-ro</td>\n",
       "      <td>te-cu-ra</td>\n",
       "      <td>tecura</td>\n",
       "      <td>tecura</td>\n",
       "      <td>0.298652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>se-gu-ro</td>\n",
       "      <td>te-vu-to</td>\n",
       "      <td>tevuto</td>\n",
       "      <td>tevuto</td>\n",
       "      <td>0.419774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>se-gu-ro</td>\n",
       "      <td>te-cu-no</td>\n",
       "      <td>tecuno</td>\n",
       "      <td>tecuno</td>\n",
       "      <td>0.577307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>se-gu-ro</td>\n",
       "      <td>te-ru-ra</td>\n",
       "      <td>terura</td>\n",
       "      <td>terura</td>\n",
       "      <td>0.611347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>se-gu-ro</td>\n",
       "      <td>te-du-ra</td>\n",
       "      <td>tedura</td>\n",
       "      <td>tedura</td>\n",
       "      <td>0.617744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>se-gu-ro</td>\n",
       "      <td>te-cu-to</td>\n",
       "      <td>tecuto</td>\n",
       "      <td>tecuto</td>\n",
       "      <td>0.861067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>se-gu-ro</td>\n",
       "      <td>te-vu-ra</td>\n",
       "      <td>tevura</td>\n",
       "      <td>tevura</td>\n",
       "      <td>0.941961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>se-gu-ro</td>\n",
       "      <td>te-vu-no</td>\n",
       "      <td>tevuno</td>\n",
       "      <td>tevuno</td>\n",
       "      <td>0.948716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word     Match Nonword Nonword_Concat      Rand\n",
       "7  se-gu-ro  te-ru-to  teruto         teruto  0.129886\n",
       "8  se-gu-ro  te-ru-no  teruno         teruno  0.143047\n",
       "0  se-gu-ro  te-cu-ra  tecura         tecura  0.298652\n",
       "4  se-gu-ro  te-vu-to  tevuto         tevuto  0.419774\n",
       "2  se-gu-ro  te-cu-no  tecuno         tecuno  0.577307\n",
       "6  se-gu-ro  te-ru-ra  terura         terura  0.611347\n",
       "9  se-gu-ro  te-du-ra  tedura         tedura  0.617744\n",
       "1  se-gu-ro  te-cu-to  tecuto         tecuto  0.861067\n",
       "3  se-gu-ro  te-vu-ra  tevura         tevura  0.941961\n",
       "5  se-gu-ro  te-vu-no  tevuno         tevuno  0.948716"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTemp = df.loc[i:j,:]\n",
    "#dfTemp.sort_values(by=['Rand'])\n",
    "df_sorted = dfTemp.sort_values(by='Rand',ascending = True)\n",
    "#print(len(dfTemp))\n",
    "#dfTemp.dtypes\n",
    "df_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teruto\n"
     ]
    }
   ],
   "source": [
    "token = df_sorted.iat[0,3]\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(nonwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['teruto']\n"
     ]
    }
   ],
   "source": [
    "nonwords.append(token)\n",
    "print(nonwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-7554ff895b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Rand'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnonwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   2531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m             \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iget_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2533\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_box_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2535\u001b[0m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "nonwords = []\n",
    "i = 0\n",
    "j = 9\n",
    "while j < 900:\n",
    "    df_temp = df.loc[i:j,:]\n",
    "    df_sorted = df_temp.sort_values(by='Rand',ascending = True)\n",
    "    token = df_sorted.iat[0,3]\n",
    "    nonwords.append(token)\n",
    "    i += 10\n",
    "    j += 10\n",
    "print(nonwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teruto 0 9 0\n",
      "lodato 10 19 0\n",
      "socano 20 29 0\n",
      "ruceba 30 39 0\n",
      "comoro 40 49 0\n",
      "valito 50 59 0\n",
      "merido 60 69 0\n",
      "todeca 70 79 0\n",
      "secoco 80 89 0\n",
      "deceja 90 99 0\n",
      "tacuro 100 109 0\n",
      "cemoca 110 119 0\n",
      "nosala 120 129 0\n",
      "ramena 130 139 0\n",
      "sadido 140 149 0\n",
      "demasa 150 159 0\n",
      "rarido 160 169 0\n",
      "tevido 170 179 0\n",
      "tasivo 180 189 0\n",
      "fasama 190 199 0\n",
      "sebada 200 209 0\n",
      "dodino 210 219 0\n",
      "cavaco 220 229 0\n",
      "nemada 230 239 0\n",
      "soredo 240 249 0\n",
      "seciro 250 259 0\n",
      "facido 260 269 0\n",
      "tamuda 270 279 0\n",
      "maroña 280 289 0\n",
      "vunino 290 299 0\n",
      "cacato 300 309 0\n",
      "sicezo 310 319 0\n",
      "nalana 320 329 0\n",
      "saseta 330 339 0\n",
      "pamapo 340 349 0\n",
      "socido 350 359 0\n",
      "sarapa 360 369 0\n",
      "toleno 370 379 0\n",
      "devido 380 389 0\n",
      "linido 390 399 0\n",
      "laldon 400 409 0\n",
      "firtum 410 419 0\n",
      "vurnar 420 429 0\n",
      "rorcon 430 439 0\n",
      "nongal 440 449 0\n",
      "murdon 450 459 0\n",
      "rextor 460 469 0\n",
      "cansin 470 479 0\n",
      "marnel 480 489 0\n",
      "vesvor 490 499 0\n",
      "larton 500 509 0\n",
      "carsol 510 519 0\n",
      "culbor 520 529 0\n",
      "desbez 530 539 0\n",
      "masday 540 549 0\n",
      "montal 550 559 0\n",
      "repcel 560 569 0\n",
      "banten 570 579 0\n",
      "bembor 580 589 0\n",
      "purnar 590 599 0\n",
      "jarnen 600 609 0\n",
      "cuntor 610 619 0\n",
      "pestal 620 629 0\n",
      "martor 630 639 0\n",
      "penril 640 649 0\n",
      "laxtor 650 659 0\n",
      "pestal 660 669 0\n",
      "pistal 660 669 1\n",
      "canvon 670 679 0\n",
      "castes 680 689 0\n",
      "gespan 690 699 0\n",
      "bumpon 700 709 0\n",
      "jucmin 710 719 0\n",
      "dexxin 720 729 0\n",
      "naldon 730 739 0\n",
      "mostel 740 749 0\n",
      "pusnon 750 759 0\n",
      "ralbon 760 769 0\n",
      "sixtan 770 779 0\n",
      "soxtil 780 789 0\n",
      "vuzcan 790 799 0\n",
      "['teruto', 'lodato', 'socano', 'ruceba', 'comoro', 'valito', 'merido', 'todeca', 'secoco', 'deceja', 'tacuro', 'cemoca', 'nosala', 'ramena', 'sadido', 'demasa', 'rarido', 'tevido', 'tasivo', 'fasama', 'sebada', 'dodino', 'cavaco', 'nemada', 'soredo', 'seciro', 'facido', 'tamuda', 'maroña', 'vunino', 'cacato', 'sicezo', 'nalana', 'saseta', 'pamapo', 'socido', 'sarapa', 'toleno', 'devido', 'linido', 'laldon', 'firtum', 'vurnar', 'rorcon', 'nongal', 'murdon', 'rextor', 'cansin', 'marnel', 'vesvor', 'larton', 'carsol', 'culbor', 'desbez', 'masday', 'montal', 'repcel', 'banten', 'bembor', 'purnar', 'jarnen', 'cuntor', 'pestal', 'martor', 'penril', 'laxtor', 'pistal', 'canvon', 'castes', 'gespan', 'bumpon', 'jucmin', 'dexxin', 'naldon', 'mostel', 'pusnon', 'ralbon', 'sixtan', 'soxtil', 'vuzcan']\n"
     ]
    }
   ],
   "source": [
    "nonwords = []\n",
    "i = 0\n",
    "j = 9\n",
    "k = 0\n",
    "try:\n",
    "    while j < 800:\n",
    "        df_temp = df.loc[i:j,:]\n",
    "        df_sorted = df_temp.sort_values(by='Rand',ascending = True)\n",
    "        token = df_sorted.iat[k,3]\n",
    "        print(token,i,j,k)\n",
    "        if token in nonwords:\n",
    "            k += 1\n",
    "        else:\n",
    "            nonwords.append(token)\n",
    "            i += 10\n",
    "            j += 10\n",
    "            k = 0\n",
    "except IndexError:\n",
    "    print(\"Upper bound reached\")\n",
    "finally:\n",
    "    print(nonwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['teruto', 'teruto', 'lodato', 'socano', 'ruceba', 'comoro', 'valito', 'merido', 'todeca', 'secoco', 'deceja', 'tacuro', 'cemoca', 'nosala', 'ramena', 'sadido', 'demasa', 'rarido', 'tevido', 'tasivo', 'fasama', 'sebada', 'dodino', 'cavaco', 'nemada', 'soredo', 'seciro', 'facido', 'tamuda', 'maroña', 'vunino', 'cacato', 'sicezo', 'nalana', 'saseta', 'pamapo', 'socido', 'sarapa', 'toleno', 'devido', 'linido', 'laldon', 'firtum', 'vurnar', 'rorcon', 'nongal', 'murdon', 'rextor', 'cansin', 'marnel', 'vesvor', 'larton', 'carsol', 'culbor', 'desbez', 'masday', 'montal', 'repcel', 'banten', 'bembor', 'purnar', 'jarnen', 'cuntor', 'pestal', 'martor', 'penril', 'laxtor', 'pestal', 'canvon', 'castes', 'gespan', 'bumpon', 'jucmin', 'dexxin', 'naldon', 'mostel', 'pusnon', 'ralbon', 'sixtan', 'soxtil', 'vuzcan']\n"
     ]
    }
   ],
   "source": [
    "print(nonwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809\n"
     ]
    }
   ],
   "source": [
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "print(len(nonwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Good Stuff\n",
    "The code block below in this Jupyter Notebook takes the excel file containing nonwords created by the Wuggy.app application and generates a list of nonwords ready for the PyschoPy experiment.\n",
    "\n",
    "## Wuggy.app\n",
    "Critical items were taken from Carreiras and Perea (2002) experiment 4. All the critical items fell into one of two structures CV.CV.CV or CVC.CVC where 40 were of the first type and 40 were of the second type for a total of 80 critical items.\n",
    "\n",
    "This list of 80 critical items were entered into Wuggy.app as words and the second column gave Wuggy.app the correction Spanish syllabification of each critical word—generated from within the application. I then used the verify feature to ensure all data was correct. This resulted in one error because \"malena\" was not found in the Wuggy.app dictionary. Therefore, I gave the correct syllabification of \"ma-le-na\" where hypens represent syllable boundaries. I then re-verified the data before runnning the analysis. Once I receive no errors, I ran the following analysis to generate new nonwords. \n",
    "\n",
    "### Parameters:\n",
    "\n",
    "**General Settings:**\n",
    "* Language module: Orthographic Spanish\n",
    "* Output type: Only pseudowords\n",
    "* Maximal number of candidates: 10 per word\n",
    "* Maximal search time per word: 10 seconds\n",
    "    \n",
    "**Output Restrictions:**\n",
    "* Checked: Match length of subsyllabic segements\n",
    "* Checked: Match letter length\n",
    "* Checked: Match transition frequencies (concentric search)\n",
    "* Checked: Match subsyllabic segments: \"2\" out of \"3\"\n",
    "        \n",
    "**Output Options:**\n",
    "* *Syllables* chosen from dropdown box\n",
    "* Checked: Lexicality\n",
    "* Checked: OLD20\n",
    "* Checked: Neighbors at edit distance 1\n",
    "* Checked: Number of overlapping segments\n",
    "* Checked: Deviation statistics\n",
    "        \n",
    "### Output file\n",
    "\n",
    "Wuggy outputs a file that contains all the raw data that was then imported into Microsoft Excel to preserve all the unicode characters contained in the data. **Do not simply open this from Windows Explorer or Finder** as it will replace all encoding of the file eliminating special characters.\n",
    "\n",
    "**WARNING:** Wuggy uses syllabification and it removes the accented vowels in Spanish. The only way to remedy this problem is to fix it once it has been imported into Excel. \n",
    "\n",
    "Once in Excel, I renamed the worksheet *WuggyRawOutput*. Then I duplicated this worksheet and renamed the new worksheet *WuggyBuilt*. I then deleted all the columns except for columns A and B (\"Word\" and \"Match\"). I then added two columns called \"Nonword_Concat\" and \"Rand\".\n",
    "In the *Nonword_Concat* column, I entered the following formulas:\n",
    "\n",
    "* For CV-CV-CV words: =CONCATENATE(LEFT(B2,2),MID(B2,4,2),MID(B2,7,2))\n",
    "    * This formula combines the two leftmost characters, skips character 3 (the first syllable boundary marker), adds the 4th and 5th characters to the string store, skips the 6th character (the second syllable boundary marker), and finally adds the 7th and 8th characters from the left to create the six-letter nonword.\n",
    "\n",
    "\n",
    "* For CVC-CVC words: =CONCATENATE(LEFT(B403,3),RIGHT(B403,3))\n",
    "    * This formula combines the three leftmost and three rightmost characters to create the six-letter nonword excluding the one and only syllable boundary marker.\n",
    "\n",
    "These two formulas are necessary in order to create nonwords without the syllable markers. Once these steps have been completed, the file is ready for the python code housed in the cell below.\n",
    "\n",
    "## Python Code\n",
    "\n",
    "The python code here first imports the packages and functions needed to excute all the code correctly:\n",
    "\n",
    "* pandas - an package to manipulate tabular data\n",
    "* segFunctions - a function that creates csv files extracted from an Excel Workbook.\n",
    "\n",
    "The script then calls segFunctions to create a temporary csv file that is then read in and stored as *df*. I then create an empty list to store the words I will extract with the script and setup three different counters. *i* is the counter that I will use reference the rows (minimum range value) in my dataframe and since python is zero-based, I set its default value to zero (0). *j* is the counter that I will use to reference the rows (maximum range value). **Note: this only works when the maximum number of candidates of 10 was actually reached when the Wuggy.app analysis ran** *This can be checked by multiplying the number of words submitted to Wuggy.app by 10. k* is the counter that will be used to reference the first row of the sorted dataframes and will only increase when duplicate nonword entries are found. Once the it has resolved the nonword duplicates it is reset to zero(0) for the next iteration.\n",
    "\n",
    "Once the preamble is squared away and checks have been made to ensure datastructure is accurate, the meat of the code is executed. While *j* is less the 800, *df_temp* is stored as all columns from *df* with rows between *i* and *j*. Then *df_sorted* stores a new dataframe organized in ascending order according to the *Rand* column, which contains random numbers between 0 and 1, generated from Excel. Using the *df_sorted* dataframe, *token* stores the value from row *k*,zero(0) by default, and column 3. If the token is found in the list *nonwords*, it increases the value of *k* by one to go to the next row and tests it again. This process occurs until *token* is not found in the list. When *token* does not exist in the list, it appends the value of *token* to the list *nonwords* and then increases the values of *i and j* by 10 to get to the next set of data. This process repeats through the entire dataframe. In the current state, the length of the list *nonwords* should be 80. This can be checked using the following code snippet:\n",
    "\n",
    "```len(nonwords)```\n",
    "\n",
    "Following all the iterations, the list is added to a dataframe called *df_nonwords*, which is then passed through to a csv file stored in the *./processed_data/exp_files/* directory for later use.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import segFunctions\n",
    "\n",
    "# runs the csv_from_excel function:\n",
    "# creates a csv for pulling out random selections by word\n",
    "segFunctions.csv_from_excel('LexicalNonwordStats_Wuggy.xlsx','WuggyBuilt','./data/temp_data/nonwords_all.csv')\n",
    "\n",
    "# reads in the csv created by csv_from_excel function\n",
    "df = pd.read_csv('./data/temp_data/nonwords_all.csv')\n",
    "\n",
    "nonwords = [] #creates an empty list for appending nonwords\n",
    "i = 0 # counter used in row delimiter of dataframe\n",
    "j = 9 # counter used in column delimiter of dataframe\n",
    "k = 0 # counter used to iterate to next item when duplicate found\n",
    "\n",
    "#iterate through the dataframe of 800 nonwords generated with Wuggy.app\n",
    "try:\n",
    "    while j < 800: # stop iteration before index value is exceeded\n",
    "        df_temp = df.loc[i:j,:] # subset dataframe by rows with all columns\n",
    "        df_sorted = df_temp.sort_values(by='Rand',ascending = True) # sort ascending by Rand column\n",
    "        token = df_sorted.iat[k,3] # store the value of cell in dataframe\n",
    "        if token in nonwords: # check to see if word is already in list\n",
    "            k += 1 # when duplicate found go to next row value and try again\n",
    "        else: # no duplicate found run this\n",
    "            nonwords.append(token) # append the new token to the list 'nonwords'\n",
    "            i += 10 #increase by 10 to get to next dataset\n",
    "            j += 10 #increase by 10 to get to next dataset\n",
    "            k = 0 #reset row indexer to zero(0) \n",
    "except IndexError: # check to see that dataframe length has not been exceeded\n",
    "    print(\"Upper bound reached\") # print debug message\n",
    "finally: # once everything is completed, print the list\n",
    "    # passes the list to a dataframe, which is then written to a csv file\n",
    "    df_nonwords = pd.DataFrame(data={\"col1\": nonwords})\n",
    "    df_nonwords.to_csv('./data/processed_data/exp_files/nonwords_list.csv',header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word     Match Nonword Nonword_Concat      Rand\n",
      "0    se-gu-ro  te-cu-ra  tecura         tecura  0.298652\n",
      "1    se-gu-ro  te-cu-to  tecuto         tecuto  0.861067\n",
      "2    se-gu-ro  te-cu-no  tecuno         tecuno  0.577307\n",
      "3    se-gu-ro  te-vu-ra  tevura         tevura  0.941961\n",
      "4    se-gu-ro  te-vu-to  tevuto         tevuto  0.419774\n",
      "5    se-gu-ro  te-vu-no  tevuno         tevuno  0.948716\n",
      "6    se-gu-ro  te-ru-ra  terura         terura  0.611347\n",
      "7    se-gu-ro  te-ru-to  teruto         teruto  0.129886\n",
      "8    se-gu-ro  te-ru-no  teruno         teruno  0.143047\n",
      "9    se-gu-ro  te-du-ra  tedura         tedura  0.617744\n",
      "10   bo-ni-to  to-sa-to  tosato         tosato  0.712559\n",
      "11   bo-ni-to  to-ma-to  tomato         tomato  0.999413\n",
      "12   bo-ni-to  vo-ca-to  vocato         vocato  0.856972\n",
      "13   bo-ni-to  vo-ra-to  vorato         vorato  0.284149\n",
      "14   bo-ni-to  vo-da-to  vodato         vodato  0.417282\n",
      "15   bo-ni-to  vo-sa-to  vosato         vosato  0.707577\n",
      "16   bo-ni-to  vo-ma-to  vomato         vomato  0.384355\n",
      "17   bo-ni-to  lo-ca-to  locato         locato  0.724578\n",
      "18   bo-ni-to  lo-ra-to  lorato         lorato  0.951074\n",
      "19   bo-ni-to  lo-da-to  lodato         lodato  0.099889\n",
      "20   mo-li-no  so-ra-no  sorano         sorano  0.836723\n",
      "21   mo-li-no  ro-ra-no  rorano         rorano  0.580824\n",
      "22   mo-li-no  po-ra-no  porano         porano  0.469855\n",
      "23   mo-li-no  so-ci-ro  sociro         sociro  0.722870\n",
      "24   mo-li-no  so-ci-co  socico         socico  0.524791\n",
      "25   mo-li-no  so-ci-to  socito         socito  0.991930\n",
      "26   mo-li-no  so-ca-no  socano         socano  0.282590\n",
      "27   mo-li-no  so-ri-ro  soriro         soriro  0.655552\n",
      "28   mo-li-no  so-ri-co  sorico         sorico  0.427373\n",
      "29   mo-li-no  so-ri-to  sorito         sorito  0.597146\n",
      "..        ...       ...     ...            ...       ...\n",
      "770   sul-tan   ruo-tan  ruotan         ruotan  0.960228\n",
      "771   sul-tan   rua-tan  ruatan         ruatan  0.682226\n",
      "772   sul-tan   rup-tan  ruptan         ruptan  0.318216\n",
      "773   sul-tan   ruc-tan  ructan         ructan  0.456428\n",
      "774   sul-tan   rel-tan  reltan         reltan  0.396288\n",
      "775   sul-tan   ril-tan  riltan         riltan  0.802516\n",
      "776   sul-tan   rol-tan  roltan         roltan  0.511204\n",
      "777   sul-tan   siu-tan  siutan         siutan  0.309437\n",
      "778   sul-tan   sii-tan  siitan         siitan  0.818225\n",
      "779   sul-tan   six-tan  sixtan         sixtan  0.147577\n",
      "780   tex-til   tap-til  taptil         taptil  0.872208\n",
      "781   tex-til   sew-til  sewtil         sewtil  0.886536\n",
      "782   tex-til   sep-til  septil         septil  0.576109\n",
      "783   tex-til   sax-til  saxtil         saxtil  0.756067\n",
      "784   tex-til   sox-til  soxtil         soxtil  0.571732\n",
      "785   tex-til   taf-til  taftil         taftil  0.838003\n",
      "786   tex-til   top-til  toptil         toptil  0.869757\n",
      "787   tex-til   sec-til  sectil         sectil  0.755976\n",
      "788   tex-til   sel-til  seltil         seltil  0.900759\n",
      "789   tex-til   six-til  sixtil         sixtil  0.827700\n",
      "790   vol-can   vul-dan  vuldan         vuldan  0.060049\n",
      "791   vol-can   vua-can  vuacan         vuacan  0.676950\n",
      "792   vol-can   vos-dan  vosdan         vosdan  0.375450\n",
      "793   vol-can   dul-can  dulcan         dulcan  0.586945\n",
      "794   vol-can   ful-can  fulcan         fulcan  0.348008\n",
      "795   vol-can   fel-can  felcan         felcan  0.166801\n",
      "796   vol-can   fil-can  filcan         filcan  0.862816\n",
      "797   vol-can   fol-dan  foldan         foldan  0.458116\n",
      "798   vol-can   fop-can  fopcan         fopcan  0.249141\n",
      "799   vol-can   vuz-can  vuzcan         vuzcan  0.032408\n",
      "\n",
      "[800 rows x 5 columns]\n",
      "19\n",
      "29\n",
      "39\n",
      "49\n",
      "59\n",
      "69\n",
      "79\n",
      "89\n",
      "99\n",
      "109\n",
      "119\n",
      "129\n",
      "139\n",
      "149\n",
      "159\n",
      "169\n",
      "179\n",
      "189\n",
      "199\n",
      "209\n",
      "219\n",
      "229\n",
      "239\n",
      "249\n",
      "259\n",
      "269\n",
      "279\n",
      "289\n",
      "299\n",
      "309\n",
      "319\n",
      "329\n",
      "339\n",
      "349\n",
      "359\n",
      "369\n",
      "379\n",
      "389\n",
      "399\n",
      "409\n",
      "419\n",
      "429\n",
      "439\n",
      "449\n",
      "459\n",
      "469\n",
      "479\n",
      "489\n",
      "499\n",
      "509\n",
      "519\n",
      "529\n",
      "539\n",
      "549\n",
      "559\n",
      "569\n",
      "579\n",
      "589\n",
      "599\n",
      "609\n",
      "619\n",
      "629\n",
      "639\n",
      "649\n",
      "659\n",
      "669\n",
      "669\n",
      "679\n",
      "689\n",
      "699\n",
      "709\n",
      "719\n",
      "729\n",
      "739\n",
      "749\n",
      "759\n",
      "769\n",
      "779\n",
      "789\n",
      "799\n",
      "809\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import segFunctions\n",
    "\n",
    "# runs the csv_from_excel function:\n",
    "# creates a csv for pulling out random selections by word\n",
    "segFunctions.csv_from_excel('LexicalNonwordStats_Wuggy.xlsx','WuggyBuilt','./data/temp_data/nonwords_all.csv')\n",
    "\n",
    "# reads in the csv created by csv_from_excel function\n",
    "df = pd.read_csv('./data/temp_data/nonwords_all.csv')\n",
    "print(df)\n",
    "nonwords = [] #creates an empty list for appending nonwords\n",
    "i = 0 # counter used in row delimiter of dataframe\n",
    "j = 9 # counter used in column delimiter of dataframe\n",
    "k = 0 # counter used to iterate to next item when duplicate found\n",
    "\n",
    "#iterate through the dataframe of 800 nonwords generated with Wuggy.app\n",
    "try:\n",
    "    while j < len(df): # stop iteration before index value is exceeded\n",
    "        df_temp = df.loc[i:j,:] # subset dataframe by rows with all columns\n",
    "        df_sorted = df_temp.sort_values(by='Rand',ascending = True) # sort ascending by Rand column\n",
    "        token = df_sorted.iat[k,3] # store the value of cell in dataframe\n",
    "        if token in nonwords: # check to see if word is already in list\n",
    "            k += 1 # when duplicate found go to next row value and try again\n",
    "        else: # no duplicate found run this\n",
    "            nonwords.append(token) # append the new token to the list 'nonwords'\n",
    "            i += 10 #increase by 10 to get to next dataset\n",
    "            j += 10 #increase by 10 to get to next dataset\n",
    "            k = 0 #reset row indexer to zero(0)\n",
    "        print(j)\n",
    "        \n",
    "except IndexError: # check to see that dataframe length has not been exceeded\n",
    "    print('Upper bound reached')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d', 'r', 'W', 'o'}\n"
     ]
    }
   ],
   "source": [
    "set_test = set(df.columns.values[0])\n",
    "print(set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonword_Concat\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.values[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Word', 'Match', 'Nonword'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(df.columns.values[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Match', 'Nonword', 'Nonword_Concat', 'Rand', 'Word'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C', 'N', '_', 'a', 'c', 'd', 'n', 'o', 'r', 't', 'w'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.columns.values[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['se-gu-ro', 'bo-ni-to', 'mo-li-no', 'pu-re-za', 'co-ro-na', 'ba-ra-to', 'pe-sa-do', 'do-ce-na', 'pe-no-so', 're-ce-lo', 'ba-su-ra', 'ce-lo-so', 'go-ri-la', 'ma-le-ta', 'pa-ra-do', 're-sa-ca', 'sa-la-do', 'se-na-do', 'ta-ma-ño', 'va-si-ja', 'me-di-da', 'ro-ma-no', 'ca-si-no', 'he-ri-da', 'mo-ne-da', 'de-li-to', 'ga-na-do', 'sa-lu-do', 'pa-lo-ma', 'gu-sa-no', 'ca-se-ta', 'di-se-ño', 'ha-ri-na', 'ma-le-na', 'pa-si-vo', 'ro-sa-do', 'sa-li-va', 'so-ne-to', 're-ca-do', 'vi-sa-do', 'bal-con', 'vir-tud', 'vul-gar', 'rin-con', 'nor-mal', 'per-don', 'sec-tor', 'car-min', 'mar-fil', 'fer-vor', 'bas-ton', 'car-tel', 'cur-sor', 'des-liz', 'mal-dad', 'pos-tal', 'rep-til', 'sar-ten', 'tam-bor', 'pul-gar', 'jar-din', 'pin-tor', 'men-tal', 'pas-tor', 'per-fil', 'fac-tor', 'por-tal', 'car-bon', 'cor-tes', 'des-van', 'bom-bon', 'jaz-min', 'del-fin', 'hal-con', 'pas-tel', 'pul-mon', 'sal-mon', 'sul-tan', 'tex-til', 'vol-can']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import segFunctions\n",
    "\n",
    "# runs the csv_from_excel function:\n",
    "# creates a csv for pulling out random selections by word\n",
    "segFunctions.csv_from_excel('LexicalNonwordStats_Wuggy.xlsx','WuggyBuilt','./data/temp_data/nonwords_all.csv')\n",
    "\n",
    "# reads in the csv created by csv_from_excel function\n",
    "df = pd.read_csv('./data/temp_data/nonwords_all.csv')\n",
    "\n",
    "uniquewords = [] #creates an empty list for appending nonwords\n",
    "k = 0 # counter used to iterate to next item when duplicate found\n",
    "\n",
    "#print(k)\n",
    "while k < len(df): # stop iteration before index value is exceeded\n",
    "    token = df.iat[k,0] # store the value of cell in dataframe\n",
    "    if token in uniquewords: # check to see if word is already in list\n",
    "        k += 1\n",
    "        continue\n",
    "    else: # no duplicate found run this\n",
    "        uniquewords.append(token) # append the new token to the list 'nonwords'\n",
    "        #print(k, token)\n",
    "        k += 1 #reset row indexer to zero(0)\n",
    "print(uniquewords)\n",
    "print(len(uniquewords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
